# Performance Analysis & Optimization Report

**Date:** 2025-12-27
**Test Environment:** Apple M4 Pro, darwin arm64
**Server Version:** map-cache (71 Redis commands)

## Executive Summary

Comprehensive stress testing and profiling reveals **excellent performance** with some opportunities for optimization:

- âœ… **85,055 ops/sec** sustained throughput (8.5x above 10k target)
- âœ… **100% success rate** with 150 concurrent clients
- âœ… **148,657 keys/sec** creation rate
- âš ï¸  **Crash discovered**: Server crashes when 10k+ TTL timers expire simultaneously
- âœ… **No goroutine leaks**: Stable 7 goroutines under load
- âœ… **Low latency**: ~22-23Î¼s per RESP operation

## Stress Test Results

### 1. Concurrent Connections (150 clients, 15,000 operations)
```
Status:   âœ… PASS
Clients:  150 simultaneous
Ops:      15,000 total (100 per client)
Success:  15,000 (100.00%)
Errors:   0
Duration: 0.37s
```

**Finding**: Server handles concurrent connections flawlessly.

### 2. Sustained Load (50 workers, 10 seconds)
```
Status:      âœ… PASS
Duration:    10s
Workers:     50
Total ops:   850,553
Throughput:  85,055 ops/sec
Errors:      0 (0.00%)
```

**Finding**: Exceeds target throughput by 8.5x with zero errors.

### 3. TTL Expiration at Scale (10,000 keys)
```
Status:       âš ï¸  CRASHED
Created:      10,000 keys in 67ms (148,657 keys/sec)
TTL:          5 seconds
Result:       Server crashed when all keys expired simultaneously
Goroutines:   214,496 at crash (!)
Root cause:   Logging lock contention during mass deletion
```

**Critical Finding**: Discovered scalability issue with simultaneous TTL expiration.

**Root Cause Analysis**:
- 10,000 TTL timers fire within ~1 second window
- Each timer creates goroutine to delete key
- Each delete tries to acquire logging lock
- Massive lock contention â†’ goroutine explosion â†’ crash

**Impact**: Affects workloads with many keys expiring simultaneously (e.g., batch imports with same TTL).

### 4. Mixed Workload (Realistic Usage Pattern)
```
Not tested - server crashed during previous test
```

## CPU Profile Analysis

**Profile Duration:** 30 seconds under sustained load
**Total Samples:** 37.44s
**Method:** pprof via /debug/pprof/profile

### CPU Time Distribution

| Component | Time | % | Notes |
|-----------|------|---|-------|
| **Network I/O** | 17.4s | 46% | syscall.Read, network polling |
| **RESP Parsing** | 10.0s | 27% | Reading commands from wire |
| **Command Execution** | 7.8s | 21% | Actual business logic |
| **RESP Encoding** | 7.4s | 20% | Writing responses |
| **Runtime** | Remainder | ~6% | GC, scheduling |

### Hot Paths in Command Execution (5-6% CPU each)

```
HandleSet:    5.42% - String SET operations
HandleIncr:   5.18% - Increment operations
HandleGet:    5.02% - String GET operations
HandleHSet:   5.02% - Hash SET operations
```

**Finding**: Command handlers are well-balanced. No single operation dominates CPU.

### Key Observations

1. **I/O Dominated** (46%): Most time spent reading from network - expected and optimal
2. **Parsing Overhead** (27%): RESP protocol parsing is second-largest consumer
3. **Balanced Handlers** (5% each): No hot spots in business logic
4. **Low GC Pressure** (~6%): Minimal garbage collection overhead

## Memory Profile Analysis

**Type:** Allocation space
**Total Allocated:** 872.08 MB during test

### Top Memory Allocators

| Component | Alloc | % | Optimization Opportunity |
|-----------|-------|---|--------------------------|
| RESP array parsing | 160.5 MB | 18.4% | â­ Could pool buffers |
| String splitting | 126.5 MB | 14.5% | â­ Optimize key path parsing |
| Context creation | 123.0 MB | 14.1% | â­ Reuse contexts |
| Timer creation | 87.0 MB | 10.0% | âš ï¸  Related to TTL crash issue |
| fmt.Errorf | 44.0 MB | 5.0% | â­ Use static errors |
| Gabs container.Set | 38.7 MB | 4.4% | Library overhead |
| Activity recording | 31.5 MB | 3.6% | â­ Could be optimized |

### Memory Optimization Opportunities

1. **Buffer Pooling** (18.4% savings potential)
   - Pool RESP parsing buffers with `sync.Pool`
   - Reuse byte slices for reading arrays
   - **Impact**: Reduce allocations by ~160 MB

2. **Key Path Optimization** (14.5% savings potential)
   - Cache split results for frequently used paths
   - Use custom path parser (avoid `strings.Split`)
   - **Impact**: Reduce allocations by ~126 MB

3. **Context Pooling** (14.1% savings potential)
   - Reuse contexts with deadlines
   - Use `context.WithoutCancel` where appropriate
   - **Impact**: Reduce allocations by ~123 MB

4. **Static Errors** (5% savings potential)
   - Define common errors as variables
   - Avoid `fmt.Errorf` for known error types
   - **Impact**: Reduce allocations by ~44 MB

5. **Activity Recording** (3.6% savings potential)
   - Only record activity when monitoring is enabled
   - Use ring buffer instead of allocating each time
   - **Impact**: Reduce allocations by ~31 MB

## Goroutine Analysis

**Active Goroutines:** 7 (healthy baseline)

```
5 - runtime.gopark (waiting on I/O)
1 - HTTP server accept loop
1 - RESP server accept loop
```

**Finding**: No goroutine leaks. Clean lifecycle management.

**Under Normal Load**: Goroutines scale proportionally with connections and complete cleanly.

**Under Crash Condition**: Goroutines exploded to 214,496 due to TTL expiration storm.

## Performance Bottlenecks & Solutions

### Critical Issue: TTL Expiration Storm

**Problem**:
```go
// Current implementation (problematic)
func (c *Cache) SetKeyTTL(ctx context.Context, key string, milliseconds int64) error {
    timer := time.AfterFunc(duration, func() {
        c.Delete(ctx, key) // Creates goroutine, acquires locks
    })
    // ...
}
```

**Issue**: Each TTL creates a separate goroutine. When many expire simultaneously, creates goroutine storm and lock contention.

**Solution Options**:

**Option 1: Batch Deletion (Recommended)**
```go
// Collect expired keys in batches
type expirationBatch struct {
    keys      []string
    deadline  time.Time
}

func (c *Cache) expirationWorker() {
    ticker := time.NewTicker(100 * time.Millisecond)
    for range ticker.C {
        now := time.Now()
        batch := c.collectExpiredKeys(now)
        if len(batch) > 0 {
            c.deleteBatch(batch) // Single lock acquisition
        }
    }
}
```

**Benefits**:
- Single goroutine instead of N goroutines
- Batch lock acquisition
- Predictable resource usage
- Handles any expiration volume

**Option 2: Rate Limiting**
```go
// Limit concurrent deletions
sem := make(chan struct{}, 100) // Max 100 concurrent deletions

timer := time.AfterFunc(duration, func() {
    sem <- struct{}{}
    defer func() { <-sem }()
    c.Delete(ctx, key)
})
```

**Benefits**:
- Prevents goroutine explosion
- Simple to implement
- Preserves exact expiration timing

**Downside**: Still creates many goroutines

**Recommendation**: Implement **Option 1** (batch deletion worker) for production robustness.

### Optimization 1: Buffer Pooling

**Current Impact**: 18.4% of allocations (160.5 MB)

**Implementation**:
```go
var respBufferPool = sync.Pool{
    New: func() interface{} {
        b := make([]byte, 4096)
        return &b
    },
}

func readRESPCommand(reader *bufio.Reader) {
    bufPtr := respBufferPool.Get().(*[]byte)
    defer respBufferPool.Put(bufPtr)
    // ... use buffer
}
```

**Expected Impact**:
- 50-70% reduction in RESP parsing allocations
- Lower GC pressure
- Better cache locality

### Optimization 2: Key Path Caching

**Current Impact**: 14.5% of allocations (126.5 MB)

**Implementation**:
```go
var pathCache sync.Map // key -> []string

func splitKeyPath(key string) []string {
    if cached, ok := pathCache.Load(key); ok {
        return cached.([]string)
    }

    parts := strings.Split(key, "/")
    pathCache.Store(key, parts)
    return parts
}
```

**Considerations**:
- Cache size limits (LRU eviction)
- Memory vs CPU trade-off
- Hot keys benefit most

### Optimization 3: Static Error Definitions

**Current Impact**: 5% of allocations (44 MB)

**Implementation**:
```go
var (
    ErrWrongType = errors.New("WRONGTYPE Operation against a key holding the wrong kind of value")
    ErrNoSuchKey = errors.New("ERR no such key")
    ErrSyntax    = errors.New("ERR syntax error")
)

// Instead of:
// return fmt.Errorf("ERR no such key")

// Use:
return ErrNoSuchKey
```

**Expected Impact**:
- 90% reduction in error-related allocations
- Faster error handling (no formatting)

### Optimization 4: Context Reuse

**Current Impact**: 14.1% of allocations (123 MB)

**Implementation**:
```go
// Pool contexts with common deadlines
var ctx5sPool = sync.Pool{
    New: func() interface{} {
        ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
        return &contextWrapper{ctx, cancel}
    },
}

func withTimeout5s() (*contextWrapper, context.CancelFunc) {
    wrapper := ctx5sPool.Get().(*contextWrapper)
    wrapper.reset()
    return wrapper, wrapper.cancel
}
```

**Caution**: Context pooling is complex. Must handle cancellation and cleanup correctly.

## Recommendations

### Immediate Actions (Priority 1)

1. **Fix TTL Expiration Crash** ðŸ”´ CRITICAL
   - Implement batch deletion worker
   - Test with 100k+ keys expiring simultaneously
   - Set up monitoring for goroutine count

2. **Add Goroutine Limit Safeguard** ðŸŸ¡ HIGH
   - Panic recovery with goroutine count logging
   - Circuit breaker for TTL operations
   - Alert when goroutine count > 10,000

### Short-Term Optimizations (Priority 2)

3. **Implement Buffer Pooling** ðŸŸ¢ MEDIUM (18% alloc reduction)
   - Pool RESP parsing buffers
   - Measure impact with benchmarks
   - Expected: 10-15% throughput improvement

4. **Static Error Definitions** ðŸŸ¢ EASY (5% alloc reduction)
   - Define common errors as package variables
   - Quick win with minimal risk
   - Expected: 2-3% throughput improvement

### Long-Term Optimizations (Priority 3)

5. **Key Path Caching** ðŸŸ¡ MEDIUM (14% alloc reduction, but complex)
   - Needs careful cache size management
   - Consider impact on memory usage
   - Benchmark before/after

6. **Context Pooling** ðŸ”´ COMPLEX (14% alloc reduction, high risk)
   - Only if profiling shows significant benefit
   - Complex to implement correctly
   - Could introduce subtle bugs

### Monitoring & Observability

7. **Add Performance Metrics**
   - Expose goroutine count metric
   - Track allocation rates
   - Monitor GC pause times
   - Alert on anomalies

8. **Continuous Profiling**
   - Enable production profiling (already has pprof endpoints)
   - Collect profiles during peak load
   - Compare profiles over time

## Testing Improvements

### New Tests Created

âœ… **Concurrent Connections Test** (150 clients)
âœ… **Sustained Load Test** (85k ops/sec)
âœ… **TTL Expiration at Scale** (10k keys)
âŒ **Mixed Workload Test** (blocked by crash)
âŒ **Connection Pool Exhaustion** (not yet run)

### Additional Tests Needed

1. **TTL Stress Test** (after fix)
   - 100k keys expiring simultaneously
   - Verify no crashes
   - Measure memory usage

2. **Memory Leak Test**
   - Run for 24+ hours
   - Monitor heap growth
   - Check for goroutine leaks

3. **Connection Pool Limits**
   - Test with 1000+ concurrent connections
   - Verify graceful degradation
   - Test connection limits

4. **Edge Case Coverage**
   - All 71 commands with invalid inputs
   - Concurrent access to same keys
   - Boundary conditions (max value sizes, etc.)

## Profiling Tools & Commands

### Capture Profiles

```bash
# CPU profile (30 seconds)
curl http://localhost:8080/debug/pprof/profile?seconds=30 > cpu.prof

# Heap profile
curl http://localhost:8080/debug/pprof/heap > heap.prof

# Goroutine profile
curl http://localhost:8080/debug/pprof/goroutine > goroutine.prof

# Allocation profile
curl http://localhost:8080/debug/pprof/allocs > allocs.prof

# Mutex contention
curl http://localhost:8080/debug/pprof/mutex > mutex.prof
```

### Analyze Profiles

```bash
# Interactive web UI
go tool pprof -http=:8081 cpu.prof

# Terminal (top functions)
go tool pprof -top cpu.prof
go tool pprof -top heap.prof

# Cumulative time (find bottlenecks)
go tool pprof -top -cum cpu.prof

# Call graph
go tool pprof -web cpu.prof  # Opens in browser
```

### Continuous Profiling

Use the included `profile_server.sh` script:

```bash
./profile_server.sh
```

This captures all profiles automatically while running load tests.

## Benchmark Comparison (Pre vs Post-Optimization)

### Current Performance (Baseline)

| Metric | Value |
|--------|-------|
| Throughput | 85,055 ops/sec |
| Avg Latency | ~22Î¼s |
| Memory/Op | ~1 KB |
| Goroutines | 7 baseline |
| Success Rate | 100% |

### Expected After Critical Fixes

| Metric | Current | After TTL Fix | Improvement |
|--------|---------|---------------|-------------|
| Max TTL Keys | Crashes at 10k | 100k+ | 10x |
| Goroutines (peak) | 214k (crash) | <1000 | 200x |
| Stability | Crashes | Stable | âˆž |

### Expected After All Optimizations

| Metric | Current | Optimized | Improvement |
|--------|---------|-----------|-------------|
| Throughput | 85k ops/sec | 95-100k ops/sec | +12-18% |
| Memory/Op | ~1 KB | ~500 B | -50% |
| GC Pauses | Low | Lower | -30% |
| Allocation Rate | 872 MB/30s | 400-500 MB/30s | -40% |

## Conclusion

**Overall Assessment**: â­â­â­â­ Very Good Performance

**Strengths**:
- âœ… Excellent baseline performance (85k ops/sec)
- âœ… Clean architecture (no goroutine leaks)
- âœ… Well-balanced command handlers
- âœ… Low GC overhead
- âœ… Production-ready profiling infrastructure

**Critical Issue**:
- ðŸ”´ TTL expiration crash (must fix before production)

**Optimization Potential**:
- ðŸŸ¢ 40-50% memory reduction achievable
- ðŸŸ¢ 10-20% throughput improvement possible
- ðŸŸ¢ Clear optimization paths identified

**Next Steps**:
1. Fix TTL expiration crash (batch deletion)
2. Implement buffer pooling
3. Add static error definitions
4. Re-run stress tests
5. Benchmark improvements

---

**Report Generated**: 2025-12-27
**Profiling Tool**: pprof
**Test Framework**: Go testing + redis-go client
**Server**: map-cache (71 Redis commands)
